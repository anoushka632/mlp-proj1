{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e89a69",
   "metadata": {
    "id": "rOyOkJJA7Foy"
   },
   "source": [
    "# Machine Learning in Python - Project 1\n",
    "\n",
    "Due Friday, March 11th by 5 pm.\n",
    "\n",
    "*include contributors names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9877315",
   "metadata": {
    "id": "PqPpPRE37Fo0"
   },
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21aa52e1",
   "metadata": {
    "id": "jb4qh-pi7Fo1"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "#os library version\n",
    "from pathlib import Path\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# extra modules\n",
    "from schrutepy import schrutepy\n",
    "\n",
    "##Global variables\n",
    "##Threshold total line percentage for being a \"main character\"\n",
    "P = 0.005\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d75658",
   "metadata": {
    "id": "K314dGEL7Fo1"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "*This section should include a brief introduction to the task and the data (assume this is a report you are delivering to a client). If you use any additional data sources, you should introduce them here and discuss why they were included.*\n",
    "\n",
    "*Briefly outline the approaches being used and the conclusions that you are able to draw.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde4150f",
   "metadata": {
    "id": "3P6Vdzbo7Fo2"
   },
   "source": [
    "## 2. Exploratory Data Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e15bf5",
   "metadata": {
    "id": "Arg9_dYE7Fo2"
   },
   "source": [
    "*Include a detailed discussion of the data with a particular emphasis on the features of the data that are relevant for the subsequent modeling. Including visualizations of the data is strongly encouraged - all code and plots must also be described in the write up. Think carefully about whether each plot needs to be included in your final draft - your report should include figures but they should be as focused and impactful as possible.*\n",
    "\n",
    "*Additionally, this section should also implement and describe any preprocessing / feature engineering of the data. Specifically, this should be any code that you use to generate new columns in the data frame `d`. All of this processing is explicitly meant to occur before we split the data in to training and testing subsets. Processing that will be performed as part of an sklearn pipeline can be mentioned here but should be implemented in the following section.*\n",
    "\n",
    "*All code and figures should be accompanied by text that provides an overview / context to what is being done or presented.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3149f961",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b82b1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load Data\n",
    "d = pd.read_csv(\"../the_office.csv\")\n",
    "transcripts = schrutepy.load_schrute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21782b3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46ba8181",
   "metadata": {},
   "outputs": [],
   "source": [
    "##copy original data so we don't lose it\n",
    "df = d.copy()\n",
    "\n",
    "##Note that some directors are repeated with different spellings. \n",
    "##Need to fix before any future data solutions\n",
    "wrongdir_dict = {\"Greg Daneils\": \"Greg Daniels\", \"Charles McDougal\": \"Charles McDougall\",\n",
    "                 \"Claire Scanlong\":\"Claire Scanlon\"}\n",
    "df[\"director\"].replace(wrongdir_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3abee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72006721",
   "metadata": {},
   "outputs": [],
   "source": [
    "##combining transcript data with original data to get character line information/important characters\n",
    "\n",
    "##string cols\n",
    "str_cols = (transcripts.applymap(type) == str).all(0)\n",
    "\n",
    "##Lower names, character's and lines for easier processing\n",
    "transcripts = transcripts.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "##Remove spaces in character (some are incorrectly written like \"jim \")\n",
    "transcripts['character'] = transcripts['character'].str.replace(\" \", \"\")\n",
    "transcripts['season_ep'] = transcripts['season'].astype(str) + \"_\" + transcripts['episode'].astype(str)\n",
    "\n",
    "# ##Per Episode Character lines\n",
    "# transcripts.character.unique()\n",
    "line_sum = transcripts.groupby([\"season_ep\", \"character\"]).size().reset_index(name = \"lines\") ##per character and episode lines\n",
    "line_sum = line_sum.sort_values(ascending = False, by = \"lines\")##sort by lines\n",
    "\n",
    "##Total line percentage per character\n",
    "line_perc = line_sum.groupby(\"character\").agg({\"lines\": \"sum\"}).sort_values(ascending = False, by = \"lines\")\n",
    "line_perc['percentage'] = line_perc/(line_perc.sum())\n",
    "\n",
    "##Greater than 1% line share\n",
    "main_char = line_perc.loc[line_perc['percentage'] > P].index\n",
    "\n",
    "##Top 10 line speakers (main_char)\n",
    "# main_char = (line_perc.nlargest(10, 'percentage')).index\n",
    "\n",
    "##per episode lines for main characters\n",
    "line_main = line_sum.loc[line_sum['character'].isin(main_char)]\n",
    "\n",
    "##Join main data with lines per character data\n",
    "\n",
    "##convert line data into dataframe with each row as a season/episode and each column as the main character lines\n",
    "char_lines = line_main.pivot_table(values='lines', index='season_ep', columns='character').reset_index()\n",
    "char_lines = char_lines.fillna(0) ##fill NA values with zero because they are only not present if no lines were spoken\n",
    "\n",
    "##calculate character line percentage per episode\n",
    "perc_cols = char_lines.select_dtypes(include=np.number).columns + \"_perc\" ##create column names\n",
    "char_lines[perc_cols] = char_lines.select_dtypes(include=np.number).div(d.n_lines, axis = 0) ##per row char line percentages\n",
    "\n",
    "##Create character dummy variables for if present\n",
    "main_char_dummy = [str(x) + \"_dummy\" for x in main_char]\n",
    "char_lines[main_char_dummy] = (char_lines[main_char] > 0).astype(int)\n",
    "\n",
    "##create season_ep column for d to join by\n",
    "df['season_ep'] = df['season'].astype(str) + \"_\" + df['episode'].astype(str)\n",
    "\n",
    "##join the main data with the lines per character data\n",
    "df = pd.merge(df, char_lines, on = 'season_ep', how = 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82998f17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8ccdf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Want to get a list of all writers and a list of all directors \n",
    "##Create new columns where the presence of each writer is it's own variable (one-hot encoding)\n",
    "\n",
    "# convert string to lists\n",
    "#df[[\"year\", \"month\", \"day\"]] = pd.DataFrame(d.air_date.str.split(\"-\").tolist()).astype(int)\n",
    "df[\"writer\"]                 = df[\"writer\"].str.split(\";\")\n",
    "df[\"main_chars\"]             = df[\"main_chars\"].str.split(\";\")\n",
    "df[\"director\"]               = df[\"director\"].str.split(\";\")\n",
    "\n",
    "##Now create dummy variables for each of the writers/directors (we are dropping main character dummies)\n",
    "\n",
    "#######################################################################################################\n",
    "##Functions to Modify Writer/Director##\n",
    "\n",
    "##Function to split column with list variables into dummy variable columns\n",
    "def split_col(column, df):\n",
    "    ##For each episode, check which writiers or directosr are present and put into a dictionary\n",
    "    ##Each column is a writer/direction; each row is an episode\n",
    "    ##The value of row, column is T/F for whether a writer/director is present\n",
    "    \n",
    "    ##get all writers/directors in a flat list and then get unique writers/directors\n",
    "    all_items = [item for ep_item in df[column] for item in ep_item] ##flattened list of writers/directors (duplicates)\n",
    "    items = list(set(all_items)) ##gets list of unique writers/directors\n",
    "\n",
    "    ##For each episode, check which writers/directors are present and put into a dictionary\n",
    "    ##Each column is an individual writer/director; each row is an episode\n",
    "    ##The value of row, column is T/F for whether a writer/director is present\n",
    "    items_df = pd.DataFrame(list(map(lambda x: [i in x for i in items], df[column]))) ##this method worked on all computers\n",
    "    items_df = items_df.astype(int) ##convert boolean T/F to 1/0\n",
    "    items_df.columns = [str(x) + \"_dummy\" + \"_\" + column for x in items] ##item names + \"dummy\"\n",
    "    \n",
    "    return(items_df)\n",
    "\n",
    "\n",
    "##Function to group \"low\" appearance directors/writers into one \"low-appearance\" column dummy variable\n",
    "##Removes the original columns for those low-appearane writers directors\n",
    "def categorize_low(column_type, df, threshold):\n",
    "    ##if writer/director shows up less than or equal to the threshold, re-categorize to \"Low-Appearance\"\n",
    "    items_eps = df.sum(axis = 0)\n",
    "    low_appearance = list(items_eps[items_eps <= threshold].index) \n",
    "    new_col = \"low_appearance\" + \"_\" + column_type\n",
    "\n",
    "    ##create new writer column for low-appearance\n",
    "    df[new_col] = df[low_appearance].sum(axis = 1) > 0 ##Identifies episodes where a low_appearance writer/dir is\n",
    "    df[new_col] = df[new_col].astype(int) ##convert boolean to integer\n",
    "    \n",
    "    ##drop original low_appearance columns\n",
    "    df.drop(columns = low_appearance, inplace = True)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "##End of Functions##\n",
    "##################################################################################################################\n",
    "\n",
    "##create dummy variable dataframes for writers and directors\n",
    "writers_df = split_col(column = \"writer\", df = df)\n",
    "directors_df = split_col(column = \"director\", df = df)\n",
    "\n",
    "##group low-appearance writers and directors and remove columns for those low-appearance writers/directors\n",
    "writers_df = categorize_low(column_type = \"writer\", df = writers_df, threshold = 2)\n",
    "directors_df = categorize_low(column_type = \"directors\", df = directors_df, threshold = 2)\n",
    "\n",
    "##Add the writer data to overall data\n",
    "df = pd.concat([df, writers_df, directors_df], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dec14b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ac5dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create multipart episde columns\n",
    "\n",
    "# Create column to indicate if said episode consist of multi parts\n",
    "p = re.compile(\"Parts 1&2\")\n",
    "df[\"multi_part_dummy\"] = [int(not int(pd.isnull(re.search(p,i)))) for i in df[\"episode_name\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be0a76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61d999b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Replace any spaces in column names with \"_\"\n",
    "df.columns = df.columns.str.replace(' ', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d71b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32271568",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save data and filter to remove columns we don't want\n",
    "\n",
    "##create data directory if it doesn't exist\n",
    "if not os.path.exists(\"./data\"): os.mkdir(\"./data\")\n",
    "\n",
    "# %%\n",
    "df.to_csv(\"data/full_raw_dat.csv\", index=False)\n",
    "\n",
    "# remove unused columns and observations\n",
    "col_drop = [\"episode_name\", \"season_ep\", \"air_date\", \"episode\", \"writer\", \"director\", \"main_chars\"]\n",
    "p        = re.compile(\"Part [12]\")\n",
    "row_drop = [pd.isnull(re.search(p,i)) for i in df[\"episode_name\"]]\n",
    "fdat     = df.drop(col_drop,axis=1).iloc[row_drop,:]\n",
    "\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "fdat.to_csv(\"data/full_filtered_dat.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e1625",
   "metadata": {},
   "source": [
    "**Standardizing and Splitting Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d072230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 55 ##for reproducibility\n",
    "\n",
    "#%%\n",
    "# read full data\n",
    "fdat = pd.read_csv(\"data/full_filtered_dat.csv\")\n",
    "\n",
    "# separate response and explanatory data\n",
    "X = fdat.drop(\"imdb_rating\", axis=1)\n",
    "y = fdat.imdb_rating\n",
    "\n",
    "##Creating categorical version of response data for potential classifiers instead of regressors\n",
    "cuts = [6,7,8,9,10] ##does not include the lowest; so here would make 4 bins between each of the numbers\n",
    "binnames = ['4', '3', '2', '1'] ##name of bins with 1 being the best rating\n",
    "y_class = pd.cut(y, cuts, labels = binnames)\n",
    "\n",
    "\n",
    "# binlist = [y <= 7.5, y <= 8.5, y <= 9, y <= 10] ##cutoffs for different imdb categories\n",
    "# binnames = ['1', '2', '3', \"4\"] ##names for the categories\n",
    "# y_class = pd.Series(np.select(binlist, binnames, default='unknown')) \n",
    "\n",
    "\n",
    "# np.unique(y_class, return_counts=True)\n",
    "\n",
    "# write raw files\n",
    "\n",
    "##make new file\n",
    "Path(f\"./data/seed_{SEED}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "##split data into train test\n",
    "##Numerical data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=SEED, stratify = y_class)\n",
    "\n",
    "##binned data (X_test and X_train are the same as above since we use the same seed, so no need to respecify)\n",
    "##Unpack to _ and _ for both since we made above\n",
    "_, _, y_train_cl, y_test_cl = train_test_split(X, y_class, test_size=0.33, random_state=SEED, stratify = y_class)\n",
    "\n",
    "\n",
    "\n",
    "##Write to csvs for later\n",
    "\n",
    "##numerical data\n",
    "X_train.to_csv(f\"data/SEED_{SEED}/X_train_raw.csv\", index=False)\n",
    "X_test.to_csv(f\"data/SEED_{SEED}/X_test_raw.csv\", index=False)\n",
    "y_train.to_csv(f\"data/SEED_{SEED}/y_train.csv\", index=False)\n",
    "y_test.to_csv(f\"data/SEED_{SEED}/y_test.csv\", index=False)\n",
    "\n",
    "##binned data\n",
    "y_train_cl.to_csv(f\"data/SEED_{SEED}/y_train_cl.csv\", index=False)\n",
    "y_test_cl.to_csv(f\"data/SEED_{SEED}/y_test_cl.csv\", index=False)\n",
    "\n",
    "# create pipeline for scalers\n",
    "std_scale = Pipeline([('standard', StandardScaler())])\n",
    "minmax_scale = Pipeline([('minmax', MinMaxScaler())])\n",
    "\n",
    "# select columns that require scaling\n",
    "##select dummy variables (All dummies are numeric, but binary)\n",
    "##Therefore we select from numerics, but only if labeled with dummy writer or director\n",
    "\n",
    "scale_col = X_train.select_dtypes(include = np.number) ##subset the numeric columns\n",
    "cat_cols = scale_col.filter(regex = 'dummy|writer|director').columns ##subset the columns with dummy/writer/director\n",
    "\n",
    "scale_col = [col for col in scale_col if col not in cat_cols] ##update scale_col with numeric columns not in dummy vars\n",
    "\n",
    "# scale_col = X_train.iloc[:,0:24].select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "##Define column transformers for scaling and min/max transformations\n",
    "minmax_prep = ColumnTransformer(\n",
    "        remainder='passthrough', ##keep all columns not specified in transformer\n",
    "        transformers=[\n",
    "            ('minmax', minmax_scale , scale_col), ##apply minmax_scale to scale_col\n",
    "        ])\n",
    "\n",
    "std_prep = ColumnTransformer(\n",
    "        remainder='passthrough', ##keep all columns not specified in transformer\n",
    "        transformers=[\n",
    "            ('std', std_scale , scale_col), ##apply std_scale to scale_col\n",
    "        ])\n",
    "\n",
    "\n",
    "#%%\n",
    "# fit minmax on training data\n",
    "minmax_prep.fit(X_train)\n",
    "minmax_train = pd.DataFrame(minmax_prep.transform(X_train), columns = X_train.columns)\n",
    "minmax_test = pd.DataFrame(minmax_prep.transform(X_test), columns = X_train.columns)##Transform test with fit from training\n",
    "\n",
    "##fit scaler on training data\n",
    "std_prep.fit(X_train)\n",
    "std_train = pd.DataFrame(std_prep.transform(X_train), columns = X_train.columns)\n",
    "std_test = pd.DataFrame(std_prep.transform(X_test), columns = X_train.columns) ##Transform test data with fit from training \n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "##write to csv\n",
    "minmax_train.to_csv(f\"data/SEED_{SEED}/X_train_minmax.csv\", index=False)\n",
    "minmax_test.to_csv(f\"data/SEED_{SEED}/X_test_minmax.csv\", index=False)\n",
    "std_train.to_csv(f\"data/SEED_{SEED}/X_train_std.csv\", index=False)\n",
    "std_test.to_csv(f\"data/SEED_{SEED}/X_test_std.csv\", index=False)\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebeac9",
   "metadata": {
    "id": "Ad7J-Yw67Fo3"
   },
   "source": [
    "## 3. Model Fitting and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7b140",
   "metadata": {
    "id": "EXdvOaZs7Fo3"
   },
   "source": [
    "*In this section you should detail your choice of model and describe the process used to refine and fit that model. You are strongly encouraged to explore many different modeling methods (e.g. linear regression, regression trees, lasso, etc.) but you should not include a detailed narrative of all of these attempts. At most this section should mention the methods explored and why they were rejected - most of your effort should go into describing the model you are using and your process for tuning and validatin it.*\n",
    "\n",
    "*For example if you considered a linear regression model, a classification tree, and a lasso model and ultimately settled on the linear regression approach then you should mention that other two approaches were tried but do not include any of the code or any in depth discussion of these models beyond why they were rejected. This section should then detail is the development of the linear regression model in terms of features used, interactions considered, and any additional tuning and validation which ultimately led to your final model.* \n",
    "\n",
    "*This section should also include the full implementation of your final model, including all necessary validation. As with figures, any included code must also be addressed in the text of the document.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abc0e3b",
   "metadata": {
    "id": "edQVULLU7Fo3"
   },
   "source": [
    "## 4. Discussion & Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2c7cd",
   "metadata": {
    "id": "Fi0yKPrQ7Fo4"
   },
   "source": [
    "*In this section you should provide a general overview of your final model, its performance, and reliability. You should discuss what the implications of your model are in terms of the included features, predictive performance, and anything else you think is relevant.*\n",
    "\n",
    "*This should be written with a target audience of a NBC Universal executive who is with the show and  university level mathematics but not necessarily someone who has taken a postgraduate statistical modeling course. Your goal should be to convince this audience that your model is both accurate and useful.*\n",
    "\n",
    "*Finally, you should include concrete recommendations on what NBC Universal should do to make their reunion episode a popular as possible.*\n",
    "\n",
    "*Keep in mind that a negative result, i.e. a model that does not work well predictively, that is well explained and justified in terms of why it failed will likely receive higher marks than a model with strong predictive performance but with poor or incorrect explinations / justifications.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69773259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
